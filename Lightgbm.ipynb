import lightgbm as lgb
import pandas as pd
import scipy as sp
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import csv

from functools import partial

def confusion_matrix(a, b, minRating = None, maxRating = None):

    assert(len(a) == len(b))
    
    if minRating is None:
        minRating = min(a + b)
        
    if maxRating is None:
        maxRating = max(a + b)
        
    numratings = int(maxRating - minRating + 1)
    
    confM = [[0 for i in range(numratings)]
                for j in range(numratings)]
    
    for x, y in zip(a, b):
        confM[x - minRating][y - minRating] += 1
        
    return confM


def hist(ratings, minRating=None, maxRating=None):

    if minRating is None:
        minRating = min(ratings)
        
    if maxRating is None:
        maxRating = max(ratings)
        
    numRatings = int(maxRating - minRating + 1)
    
    histRatings = [0 for x in range(numRatings)]
    
    for r in ratings:
        histRatings[r - minRating] += 1
        
    return histRatings


def qwk(y, y_pred):
     
    a_rating = y
    b_rating = y_pred
    
    minRating=None
    maxRating=None
    
    a_rating = np.array(a_rating, dtype=int)
    b_rating = np.array(b_rating, dtype=int)
    
    assert(len(a_rating) == len(b_rating))
    
    if minRating is None:
        min_a = min(a_rating)
        min_b = min(b_rating)
        minRating = min(min_a, min_b)
    if maxRating is None:
        maxRating = max(max(a_rating), max(b_rating))
    confM = confusion_matrix(a_rating, b_rating,
                                minRating, maxRating)
    numRatings = len(conf_mat)
    num_scored_items = float(len(a_rating))

    hist_rater_a = hist(a_rating, minRating, maxRating)
    hist_rater_b = hist(b_rating, minRating, maxRating)

    top = 0.0
    bottom = 0.0

    for i in range(numRatings):
        for j in range(numRatings):
            expected_count = (hist_rater_a[i] * hist_rater_b[j]
                              / num_scored_items)
            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)
            top += d * confM[i][j] / num_scored_items
            bottom += d * expected_count / num_scored_items

    return (1.0 - top / bottom)


class OptimizedRounder(object):
    def __init__(self):
        self.coef_ = 0

    def _kappa_loss(self, coef, X, y):
        X_p = np.copy(X)
        for i, pred in enumerate(X_p):
            if pred < coef[0]:
                X_p[i] = 0
            elif pred >= coef[0] and pred < coef[1]:
                X_p[i] = 1
            elif pred >= coef[1] and pred < coef[2]:
                X_p[i] = 2
            elif pred >= coef[2] and pred < coef[3]:
                X_p[i] = 3
            else:
                X_p[i] = 4

        ll = quadratic_weighted_kappa(y, X_p)
        return -ll

    def fit(self, X, y):
        loss_partial = partial(self._kappa_loss, X=X, y=y)
        initial_coef = [0.5, 1.5, 2.5, 3.5]
        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')

    def predict(self, X, coef):
        X_p = np.copy(X)
        for i, pred in enumerate(X_p):
            if pred < coef[0]:
                X_p[i] = 0
            elif pred >= coef[0] and pred < coef[1]:
                X_p[i] = 1
            elif pred >= coef[1] and pred < coef[2]:
                X_p[i] = 2
            elif pred >= coef[2] and pred < coef[3]:
                X_p[i] = 3
            else:
                X_p[i] = 4
        return X_p

    def coefficients(self):
        return self.coef_['x']


print("~/Desktop/train.csv")
alldf = pd.read_csv('~/Desktop/train.csv')
alldf = alldf.drop(columns=['Name', 'RescuerID', 'PetID', 'Description'])
train = alldf[:len(alldf)//10:]
test = alldf[:len(alldf)//10]

x_train = train.drop(columns=['AdoptionSpeed'])
x_test = test.drop(columns=['AdoptionSpeed'])
y_train = train[['AdoptionSpeed']]
y_test = test[['AdoptionSpeed']]

lgb_train = lgb.Dataset(x_train, y_train)
lgb_eval = lgb.Dataset(x_test, y_test, reference = lgb_train)


params = {
    'application' : 'regression',
    'boosting_type': 'gbdt',
    'num_class': 5,
    'objective': 'multiclass',
    'metric': 'multiclass',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0,
    'verbose_eval':100,
    'num_threads': 4

}

print('Train')

gbm = lgb.train(params,
                lgb_train,
                num_boost_round=20,
                valid_sets=lgb_eval,
                early_stopping_rounds=5)

print('Save Model')
gbm.save_model('model.txt')

print('Start prediction...')
y_pred_test = gbm.predict(x_test, num_iteration=gbm.best_iteration)

print("Y_TEST:")
print(y_test)

print("Y_PRED:!!!!!!!!!!!!!")
print(y_pred_test)

OR = OptimizedRounder()
OR.fit(y_pred_test,y_test)
coeff = OR.coefficients()
pred = OR.predict(y_pred,coeff)

kappa = qwk(y_test, pred)
print("QWK = ",kappa)

