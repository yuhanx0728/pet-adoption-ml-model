{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import os, sys, time\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "EPOCHS = 1000\n",
    "LR = 0.0001\n",
    "HIDDEN_LIST = [ [125, 250, 250, 125],\n",
    "                [250, 500, 500, 250],\n",
    "                [500, 1000, 1000, 500],\n",
    "                [125, 250, 500, 250, 125],\n",
    "                [250, 500, 1000, 500, 250],\n",
    "                [125, 250, 500, 500, 250, 125],\n",
    "                [250, 500, 1000, 1000, 500, 250] ]\n",
    "ONE_HOT = 0 # for MaturitySize, FurLength, Health\n",
    "DATA_AUG = 0\n",
    "\n",
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "state_area ={\n",
    "    41336:19102,\n",
    "    41325:9500,\n",
    "    41367:15099,\n",
    "    41401:243,\n",
    "    41415:91,\n",
    "    41324:1664,\n",
    "    41332:6686,\n",
    "    41335:36137,\n",
    "    41330:21035,\n",
    "    41380:821,\n",
    "    41327:1048,\n",
    "    41345:73631,\n",
    "    41342:124450,\n",
    "    41326:8104,\n",
    "    41361:13035\n",
    "}\n",
    "\n",
    "\n",
    "class CSVDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.data[index][-1].long().cuda()\n",
    "        return (self.data[index][:-1], label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, HIDDEN):\n",
    "        super().__init__()\n",
    "        self.HIDDEN = HIDDEN\n",
    "        if ONE_HOT:\n",
    "            initial = 753\n",
    "        else:\n",
    "            initial = 744\n",
    "        self.base_model = nn.Sequential(torch.nn.Linear(initial, self.HIDDEN[0]),\n",
    "                                        torch.nn.ReLU(),\n",
    "                                        torch.nn.Linear(self.HIDDEN[0], self.HIDDEN[1]))\n",
    "        self.classification_layer1 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                   torch.nn.Linear(self.HIDDEN[1], self.HIDDEN[2]))\n",
    "        self.classification_layer2 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                   torch.nn.Linear(self.HIDDEN[2], self.HIDDEN[3]))\n",
    "        self.output_layer = nn.Sequential(torch.nn.ReLU(),\n",
    "                                         torch.nn.Linear(HIDDEN[-1], 5))\n",
    "        if (len(self.HIDDEN) == 5):\n",
    "            self.classification_layer3 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                       torch.nn.Linear(self.HIDDEN[3], self.HIDDEN[4]))\n",
    "        elif (len(self.HIDDEN) == 6):\n",
    "            self.classification_layer3 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                       torch.nn.Linear(self.HIDDEN[3], self.HIDDEN[4]))\n",
    "            self.classification_layer4 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                       torch.nn.Linear(self.HIDDEN[4], self.HIDDEN[5]))\n",
    "    def forward(self, input):\n",
    "        # input : some number x some number matrix\n",
    "        hidden1 = self.base_model.forward(input)\n",
    "        hidden2 = self.classification_layer1(hidden1)\n",
    "        hidden3 = self.classification_layer2(hidden2)\n",
    "        if (len(self.HIDDEN) == 4):\n",
    "            output = self.output_layer(hidden3)\n",
    "        elif (len(self.HIDDEN) == 5):\n",
    "            hidden4 = self.classification_layer3(hidden3)\n",
    "            output = self.output_layer(hidden4)\n",
    "        elif (len(self.HIDDEN) == 6):\n",
    "            hidden4 = self.classification_layer3(hidden3)\n",
    "            hidden5 = self.classification_layer4(hidden4)\n",
    "            output = self.output_layer(hidden5)\n",
    "        # output : 5 adoption speeds\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.drop(['Name', 'RescuerID', 'PetID', 'Description'], axis=1)\n",
    "d = torch.FloatTensor(df.values)\n",
    "\n",
    "# Type,Name,Age,Breed1,Breed2,Gender,Color1,Color2,Color3,MaturitySize,\n",
    "#    0,   x,  1,     2,     3,     4,     5,     6,     7,           8,\n",
    "# FurLength,Vaccinated,Dewormed,Sterilized,Health,Quantity,Fee,State,RescuerID,\n",
    "#         9,        10,      11,        12,    13,      14, 15,   16,        x,      \n",
    "# VideoAmt,Description,PetID,PhotoAmt,AdoptionSpeed\n",
    "#       17,          x,    x,      18,           19\n",
    "\n",
    "nType = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "nType[df['Type'].values.astype(int)==1] = [0.,1.]\n",
    "nType[df['Type'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "idx = d[:,2]\n",
    "nBreed1 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "idx = d[:,3]\n",
    "nBreed2 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "nGender = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "nGender[df['Gender'].values.astype(int)==1] = [0.,1.]\n",
    "nGender[df['Gender'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "idx = d[:,5]\n",
    "nColor1 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "idx = d[:,6]\n",
    "nColor2 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "idx = d[:,7]\n",
    "nColor3 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "nVaccinated = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "nVaccinated[df['Vaccinated'].values.astype(int)==1] = [0.,1.]\n",
    "nVaccinated[df['Vaccinated'].values.astype(int)==2] = [1.,0.]\n",
    "nDewormed = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "nDewormed[df['Dewormed'].values.astype(int)==1] = [0.,1.]\n",
    "nDewormed[df['Dewormed'].values.astype(int)==2] = [1.,0.]\n",
    "nSterilized = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "nSterilized[df['Sterilized'].values.astype(int)==1] = [0.,1.]\n",
    "nSterilized[df['Sterilized'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "idx = d[:,16]\n",
    "nState = torch.zeros(len(idx), int(idx.max()-idx.min())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1)-idx.min(), 1.)\n",
    "\n",
    "if ONE_HOT:\n",
    "    idx = d[:,8]\n",
    "    nMaturitySize = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "    idx = d[:,9]\n",
    "    nFurLength = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "    idx = d[:,13]\n",
    "    nHealth = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "    d = torch.cat([torch.FloatTensor(nType),\n",
    "                   d[:,1:2],\n",
    "                   torch.FloatTensor(nBreed1),\n",
    "                   torch.FloatTensor(nBreed2),\n",
    "                   torch.FloatTensor(nGender),\n",
    "                   torch.FloatTensor(nColor1),\n",
    "                   torch.FloatTensor(nColor2),\n",
    "                   torch.FloatTensor(nColor3),\n",
    "                   torch.FloatTensor(nMaturitySize),\n",
    "                   torch.FloatTensor(nFurLength),\n",
    "                   torch.FloatTensor(nVaccinated),\n",
    "                   torch.FloatTensor(nDewormed),\n",
    "                   torch.FloatTensor(nSterilized),\n",
    "                   torch.FloatTensor(nHealth),\n",
    "                   d[:,13:16],\n",
    "                   torch.FloatTensor(nState),\n",
    "                   d[:,17:]\n",
    "                  ], dim=1).cuda()\n",
    "else:\n",
    "    d = torch.cat([torch.FloatTensor(nType),\n",
    "                   d[:,1:2],\n",
    "                   torch.FloatTensor(nBreed1),\n",
    "                   torch.FloatTensor(nBreed2),\n",
    "                   torch.FloatTensor(nGender),\n",
    "                   torch.FloatTensor(nColor1),\n",
    "                   torch.FloatTensor(nColor2),\n",
    "                   torch.FloatTensor(nColor3),\n",
    "                   d[:,8:10],\n",
    "                   torch.FloatTensor(nVaccinated),\n",
    "                   torch.FloatTensor(nDewormed),\n",
    "                   torch.FloatTensor(nSterilized),\n",
    "                   d[:,14:16],\n",
    "                   torch.FloatTensor(nState),\n",
    "                   d[:,17:]\n",
    "                  ], dim=1).cuda()\n",
    "# print(d, type(d), d.shape)\n",
    "random.shuffle(d)\n",
    "partition = {}\n",
    "validation = d[:len(d)//10]\n",
    "partition['validation'] = CSVDataset(validation)\n",
    "train_set = d[len(d)//10:]\n",
    "partition['train'] = CSVDataset(train_set)\n",
    "data_train_loader = data.DataLoader(partition['train'], shuffle=True, batch_size=32)\n",
    "data_val_loader = data.DataLoader(partition['validation'], batch_size=32)\n",
    "\n",
    "# data_test = CSVDataset('data/test.csv')\n",
    "# data_test_loader = data.DataLoader(data_test, shuffle=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:43<1:06:20,  4.02s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, HIDDEN):\n",
    "    print(\"Training...\")\n",
    "    training_dir = './TEST_training_{}_{}_{}'.format(len(HIDDEN), max(HIDDEN), time.time())\n",
    "    os.mkdir(training_dir)\n",
    "    os.mkdir(training_dir+'/misclassified')\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    qwk_loss = cohen_kappa_score\n",
    "    ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "    epoch = 0\n",
    "    store_epoch_loss = []\n",
    "    store_qwk_epoch_loss = []\n",
    "    store_epoch_loss_val = []\n",
    "    store_qwk_epoch_loss_val = []\n",
    "    store_epoch_acc_val = []\n",
    "    try:\n",
    "        for e in tqdm(range(EPOCHS)):\n",
    "            #scheduler.step()\n",
    "            epoch = e + 1\n",
    "            epoch_loss = 0\n",
    "            qwk_epoch_loss = 0\n",
    "            store_batch_loss = []\n",
    "            store_qwk_batch_loss = []\n",
    "            \n",
    "            for batch_num, (X, y) in enumerate(data_train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model.forward(X.cuda())\n",
    "                batch_loss = ce_loss(prediction, y)\n",
    "                batch_loss.backward()\n",
    "                qwk_batch_loss = qwk_loss(y.clone().detach().cpu().numpy(), \n",
    "                                          np.argmax(prediction.clone().detach().cpu().numpy(), axis=1), \n",
    "                                          weights=\"quadratic\")\n",
    "                optimizer.step()\n",
    "                store_batch_loss.append(batch_loss.clone().cpu())\n",
    "                store_qwk_batch_loss.append(qwk_batch_loss)\n",
    "                epoch_loss = torch.FloatTensor(store_batch_loss).mean()\n",
    "                qwk_epoch_loss = torch.FloatTensor(store_qwk_batch_loss).mean()\n",
    "                \n",
    "            store_epoch_loss.append(epoch_loss)\n",
    "            store_qwk_epoch_loss.append(qwk_epoch_loss)\n",
    "            torch.save(model.state_dict(), \"{}/checkpoint_{}.pth\".format(training_dir, epoch))\n",
    "#             plt.plot(store_epoch_loss[1:], label=\"Training Loss\")\n",
    "#             plt.plot(store_qwk_epoch_loss[1:], label=\"Training Metric(QWK)\")\n",
    "\n",
    "            model.eval()\n",
    "            epoch_loss_val = 0\n",
    "            qwk_epoch_loss_val = 0\n",
    "            epoch_acc_val = 0\n",
    "            store_batch_loss_val = []\n",
    "            store_qwk_batch_loss_val = []\n",
    "            store_batch_acc_val = []\n",
    "            misclassified_images = []\n",
    "            for batch_num, (X, y) in enumerate(data_val_loader):\n",
    "                with torch.no_grad():\n",
    "                    prediction = model.forward(X.cuda())\n",
    "                batch_loss = ce_loss(prediction, y)\n",
    "                qwk_batch_loss = qwk_loss(y.clone().detach().cpu().numpy(), \n",
    "                                          np.argmax(prediction.clone().detach().cpu().numpy(), axis=1), \n",
    "                                          weights=\"quadratic\")\n",
    "                misclassified = prediction.max(-1)[-1].squeeze().cpu() != y.cpu()\n",
    "                misclassified_images.append(X[misclassified==1].cpu())\n",
    "                batch_acc = misclassified.float().mean()\n",
    "                store_batch_loss_val.append(batch_loss)\n",
    "                store_qwk_batch_loss_val.append(qwk_batch_loss)\n",
    "                store_batch_acc_val.append(batch_acc)\n",
    "                epoch_loss_val = torch.FloatTensor(store_batch_loss_val).mean()\n",
    "                qwk_epoch_loss_val = torch.FloatTensor(store_qwk_batch_loss_val).mean()\n",
    "                epoch_acc_val = torch.FloatTensor(store_batch_acc_val).mean()\n",
    "            store_epoch_loss_val.append(epoch_loss_val)\n",
    "            store_qwk_epoch_loss_val.append(qwk_epoch_loss_val)\n",
    "            store_epoch_acc_val.append(1-epoch_acc_val)\n",
    "            plt.plot(store_epoch_loss_val[1:], label=\"Validation Loss\")\n",
    "            plt.plot(store_qwk_epoch_loss_val[1:], label=\"Validation Metric(QWK)\")\n",
    "            plt.plot(store_epoch_acc_val[1:], label=\"Validation Accuracy\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.savefig(\"{}/Loss.png\".format(training_dir))\n",
    "            plt.close()\n",
    "            if len(misclassified_images) > 0:\n",
    "                misclassified_images = np.concatenate(misclassified_images,axis=0)\n",
    "                validation_dir = training_dir+'/misclassified/checkpoint_{}'.format(epoch)\n",
    "                os.mkdir(validation_dir)\n",
    "            model.train()\n",
    "        most_acc = max(store_epoch_acc_val)\n",
    "        min_loss = min(store_epoch_loss_val)\n",
    "        qwk_max_loss = max(store_qwk_epoch_loss_val)\n",
    "        print(\"\\nHighest accuracy of {} occured at {}...\\nMinimum loss occured at {}... \\nMaximum QWK metric of {} occured at {}\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1, \n",
    "            store_epoch_loss_val.index(min_loss)+1, \n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "        with open(training_dir+\"/HYP.txt\",\"w+\") as f:\n",
    "            f.write(\"EPOCH = {} \\n\".format(EPOCHS))\n",
    "            f.write(\"LR = {} \\n\".format(LR))\n",
    "            f.write(\"HIDDEN_LAYERS = {} \\n\".format(HIDDEN))\n",
    "            f.write(\"ONE_HOT = {} \\n\".format(ONE_HOT))\n",
    "            f.write(\"DATA_AUG = {} \\n\".format(DATA_AUG))\n",
    "            f.write(\"Highest accuracy of {} occured at {}...\\nMinimum loss of {} occured at {}... \\nMaximum QWK metric of {} occured at {}\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1, \n",
    "            min_loss, store_epoch_loss_val.index(min_loss)+1, \n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "        checkpoints = os.listdir(training_dir)\n",
    "        for checkpoint in checkpoints:\n",
    "            if \"checkpoint\" in checkpoint:\n",
    "                checkpoint_num = int(checkpoint[checkpoint.index(\"_\")+1:checkpoint.index(\".\")])\n",
    "                if checkpoint_num not in [store_qwk_epoch_loss_val.index(qwk_max_loss)+1,\n",
    "                                          store_epoch_loss_val.index(min_loss)+1,\n",
    "                                          store_epoch_acc_val.index(most_acc)+1]:\n",
    "                    os.remove(training_dir+\"/\"+checkpoint)        \n",
    "    except KeyboardInterrupt:\n",
    "        most_acc = max(store_epoch_acc_val)\n",
    "        min_loss = min(store_epoch_loss_val)\n",
    "        qwk_max_loss = max(store_qwk_epoch_loss_val)\n",
    "        print(\"\\nHighest accuracy of {} occured at {}...\\nMinimum loss of {} occured at {}... \\nMaximum QWK metric of {} occured at {}\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1, \n",
    "            min_loss, store_epoch_loss_val.index(min_loss)+1, \n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "        with open(training_dir+\"/HYP.txt\",\"w+\") as f:\n",
    "            f.write(\"EPOCH = {} \\n\".format(EPOCHS))\n",
    "            f.write(\"LR = {} \\n\".format(LR))\n",
    "            f.write(\"HIDDEN_LAYERS = {} \\n\".format(HIDDEN))\n",
    "            f.write(\"ONE_HOT = {} \\n\".format(ONE_HOT))\n",
    "            f.write(\"DATA_AUG = {} \\n\".format(DATA_AUG))\n",
    "            f.write(\"Highest accuracy of {} occured at {}...\\nMinimum loss of {} occured at {}... \\nMaximum QWK metric of {} occured at {}\\n\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1,\n",
    "            min_loss, store_epoch_loss_val.index(min_loss)+1,\n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "            f.write(\"TRAINING INCOMPLETE\")\n",
    "        checkpoints = os.listdir(training_dir)\n",
    "        for checkpoint in checkpoints:\n",
    "            if \"checkpoint\" in checkpoint:\n",
    "                checkpoint_num = int(checkpoint[checkpoint.index(\"_\")+1:checkpoint.index(\".\")])\n",
    "                if checkpoint_num not in [store_qwk_epoch_loss_val.index(qwk_max_loss)+1,\n",
    "                                          store_epoch_loss_val.index(min_loss)+1,\n",
    "                                          store_epoch_acc_val.index(most_acc)+1]:\n",
    "                    os.remove(training_dir+\"/\"+checkpoint)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(Model(HIDDEN_LIST[2]).cuda(), HIDDEN_LIST[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.806980e+02 5.462141e+01 8.104000e+03]\n",
      " [1.900750e+02 1.674621e+01 2.430000e+02]\n",
      " [2.806980e+02 5.462141e+01 8.104000e+03]\n",
      " ...\n",
      " [2.806980e+02 5.462141e+01 8.104000e+03]\n",
      " [1.166790e+02 3.348283e+01 1.910200e+04]\n",
      " [4.238900e+01 1.021064e+01 6.686000e+03]]\n",
      "tensor([1.0000e+00, 0.0000e+00, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 1.0000e+02, 2.8070e+02, 5.4621e+01, 8.1040e+03, 0.0000e+00,\n",
      "        1.0000e+00, 2.0000e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import os, sys, time\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "EPOCHS = 1000\n",
    "LR = 0.0001\n",
    "HIDDEN_LIST = [ [125, 250, 250, 125],\n",
    "                [250, 500, 500, 250],\n",
    "                [500, 1000, 1000, 500],\n",
    "                [125, 250, 500, 250, 125],\n",
    "                [250, 500, 1000, 500, 250],\n",
    "                [125, 250, 500, 500, 250, 125],\n",
    "                [250, 500, 1000, 1000, 500, 250] ]\n",
    "\n",
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "state_area ={\n",
    "    41336:19102,\n",
    "    41325:9500,\n",
    "    41367:15099,\n",
    "    41401:243,\n",
    "    41415:91,\n",
    "    41324:1664,\n",
    "    41332:6686,\n",
    "    41335:36137,\n",
    "    41330:21035,\n",
    "    41380:821,\n",
    "    41327:1048,\n",
    "    41345:73631,\n",
    "    41342:124450,\n",
    "    41326:8104,\n",
    "    41361:13035\n",
    "}\n",
    "\n",
    "\n",
    "class CSVDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.data[index][-1].long().cuda()\n",
    "        return (self.data[index][:-1], label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, HIDDEN, ONE_HOT, DATA_AUG):\n",
    "        super().__init__()\n",
    "        self.HIDDEN = HIDDEN\n",
    "        if ONE_HOT:\n",
    "            if DATA_AUG:\n",
    "                initial = 664\n",
    "            else:\n",
    "                initial = 753\n",
    "        else:\n",
    "            if DATA_AUG:\n",
    "                initial = 655\n",
    "            else:\n",
    "                initial = 744\n",
    "        self.base_model = nn.Sequential(torch.nn.Linear(initial, self.HIDDEN[0]),\n",
    "                                        torch.nn.ReLU(),\n",
    "                                        torch.nn.Linear(self.HIDDEN[0], self.HIDDEN[1]))\n",
    "        self.classification_layer1 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                   torch.nn.Linear(self.HIDDEN[1], self.HIDDEN[2]))\n",
    "        self.classification_layer2 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                   torch.nn.Linear(self.HIDDEN[2], self.HIDDEN[3]))\n",
    "        self.output_layer = nn.Sequential(torch.nn.ReLU(),\n",
    "                                         torch.nn.Linear(HIDDEN[-1], 5))\n",
    "        if (len(self.HIDDEN) == 5):\n",
    "            self.classification_layer3 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                       torch.nn.Linear(self.HIDDEN[3], self.HIDDEN[4]))\n",
    "        elif (len(self.HIDDEN) == 6):\n",
    "            self.classification_layer3 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                       torch.nn.Linear(self.HIDDEN[3], self.HIDDEN[4]))\n",
    "            self.classification_layer4 = nn.Sequential(torch.nn.ReLU(),\n",
    "                                                       torch.nn.Linear(self.HIDDEN[4], self.HIDDEN[5]))\n",
    "    def forward(self, input):\n",
    "        # input : some number x some number matrix\n",
    "        hidden1 = self.base_model.forward(input)\n",
    "        hidden2 = self.classification_layer1(hidden1)\n",
    "        hidden3 = self.classification_layer2(hidden2)\n",
    "        if (len(self.HIDDEN) == 4):\n",
    "            output = self.output_layer(hidden3)\n",
    "        elif (len(self.HIDDEN) == 5):\n",
    "            hidden4 = self.classification_layer3(hidden3)\n",
    "            output = self.output_layer(hidden4)\n",
    "        elif (len(self.HIDDEN) == 6):\n",
    "            hidden4 = self.classification_layer3(hidden3)\n",
    "            hidden5 = self.classification_layer4(hidden4)\n",
    "            output = self.output_layer(hidden5)\n",
    "        # output : 5 adoption speeds\n",
    "        return output\n",
    "    \n",
    "def prep_dataset(ONE_HOT, DATA_AUG):\n",
    "    df = pd.read_csv('data/train.csv')\n",
    "    df = df.drop(['Name', 'RescuerID', 'PetID', 'Description'], axis=1)\n",
    "    d = torch.FloatTensor(df.values)\n",
    "\n",
    "    # Type,Name,Age,Breed1,Breed2,Gender,Color1,Color2,Color3,MaturitySize,\n",
    "    #    0,   x,  1,     2,     3,     4,     5,     6,     7,           8,\n",
    "    # FurLength,Vaccinated,Dewormed,Sterilized,Health,Quantity,Fee,State,RescuerID,\n",
    "    #         9,        10,      11,        12,    13,      14, 15,   16,        x,      \n",
    "    # VideoAmt,Description,PetID,PhotoAmt,AdoptionSpeed\n",
    "    #       17,          x,    x,      18,           19\n",
    "\n",
    "    # df['Type']\n",
    "    nType = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "    nType[df['Type'].values.astype(int)==1] = [0.,1.]\n",
    "    nType[df['Type'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "    # df['Breed1']\n",
    "    idx = d[:,2]\n",
    "    nBreed1 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "    # df['Breed2']\n",
    "    idx = d[:,3]\n",
    "    nBreed2 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "    # df['Gender']\n",
    "    nGender = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "    nGender[df['Gender'].values.astype(int)==1] = [0.,1.]\n",
    "    nGender[df['Gender'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "    # df['Color1']\n",
    "    idx = d[:,5]\n",
    "    nColor1 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "    # df['Color2']\n",
    "    idx = d[:,6]\n",
    "    nColor2 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "    # df['Color3']\n",
    "    idx = d[:,7]\n",
    "    nColor3 = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "\n",
    "    # df['Vaccinated']\n",
    "    nVaccinated = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "    nVaccinated[df['Vaccinated'].values.astype(int)==1] = [0.,1.]\n",
    "    nVaccinated[df['Vaccinated'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "    # df['Dewormed']\n",
    "    nDewormed = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "    nDewormed[df['Dewormed'].values.astype(int)==1] = [0.,1.]\n",
    "    nDewormed[df['Dewormed'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "    # df['Sterilized']\n",
    "    nSterilized = np.array([[0.5,0.5]]*d.size(0)).astype(float)\n",
    "    nSterilized[df['Sterilized'].values.astype(int)==1] = [0.,1.]\n",
    "    nSterilized[df['Sterilized'].values.astype(int)==2] = [1.,0.]\n",
    "\n",
    "    # df['State']\n",
    "    idx = d[:,16]\n",
    "    nState = torch.zeros(len(idx), int(idx.max()-idx.min())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1)-idx.min(), 1.)\n",
    "\n",
    "    if DATA_AUG:\n",
    "        state_data = {}\n",
    "        for k, v in state_gdp.items():\n",
    "            state_data[k] = np.array([v, state_population[k], state_area[k]]).astype(float)\n",
    "\n",
    "        nState = np.array([[0, 0, 0]]*d.size(0)).astype(float)\n",
    "        for k,v in state_data.items():\n",
    "            nState[df['State'].values.astype(int)==k] = v\n",
    "    \n",
    "    print(nState)\n",
    "    \n",
    "    if ONE_HOT:\n",
    "        idx = d[:,8]\n",
    "        nMaturitySize = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "        idx = d[:,9]\n",
    "        nFurLength = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "        idx = d[:,13]\n",
    "        nHealth = torch.zeros(len(idx), int(idx.max())+1).scatter_(1, idx.view(len(idx)).long().unsqueeze(1), 1.)[:,1:]\n",
    "        d = torch.cat([torch.FloatTensor(nType),\n",
    "                       d[:,1:2],\n",
    "                       torch.FloatTensor(nBreed1),\n",
    "                       torch.FloatTensor(nBreed2),\n",
    "                       torch.FloatTensor(nGender),\n",
    "                       torch.FloatTensor(nColor1),\n",
    "                       torch.FloatTensor(nColor2),\n",
    "                       torch.FloatTensor(nColor3),\n",
    "                       torch.FloatTensor(nMaturitySize),\n",
    "                       torch.FloatTensor(nFurLength),\n",
    "                       torch.FloatTensor(nVaccinated),\n",
    "                       torch.FloatTensor(nDewormed),\n",
    "                       torch.FloatTensor(nSterilized),\n",
    "                       torch.FloatTensor(nHealth),\n",
    "                       d[:,13:16],\n",
    "                       torch.FloatTensor(nState),\n",
    "                       d[:,17:]\n",
    "                      ], dim=1).cuda()\n",
    "    else:\n",
    "        d = torch.cat([torch.FloatTensor(nType),\n",
    "                       d[:,1:2],\n",
    "                       torch.FloatTensor(nBreed1),\n",
    "                       torch.FloatTensor(nBreed2),\n",
    "                       torch.FloatTensor(nGender),\n",
    "                       torch.FloatTensor(nColor1),\n",
    "                       torch.FloatTensor(nColor2),\n",
    "                       torch.FloatTensor(nColor3),\n",
    "                       d[:,8:10],\n",
    "                       torch.FloatTensor(nVaccinated),\n",
    "                       torch.FloatTensor(nDewormed),\n",
    "                       torch.FloatTensor(nSterilized),\n",
    "                       d[:,14:16],\n",
    "                       torch.FloatTensor(nState),\n",
    "                       d[:,17:]\n",
    "                      ], dim=1).cuda()\n",
    "    print(d[0,:])\n",
    "    random.shuffle(d)\n",
    "    partition = {}\n",
    "    validation = d[:len(d)//10]\n",
    "    partition['validation'] = CSVDataset(validation)\n",
    "    train_set = d[len(d)//10:]\n",
    "    partition['train'] = CSVDataset(train_set)\n",
    "    data_train_loader = data.DataLoader(partition['train'], shuffle=True, batch_size=32)\n",
    "    data_val_loader = data.DataLoader(partition['validation'], batch_size=32)\n",
    "    return data_train_loader, data_val_loader\n",
    "\n",
    "\n",
    "def train(model, HIDDEN, ONE_HOT, DATA_AUG, data_train_loader, data_val_loader):\n",
    "    print(\"Training...\")\n",
    "    training_dir = './training_{}+{}_{}_{}_{}'.format(ONE_HOT, DATA_AUG, len(HIDDEN), max(HIDDEN), time.time())\n",
    "    os.mkdir(training_dir)\n",
    "    os.mkdir(training_dir+'/misclassified')\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    qwk_loss = cohen_kappa_score\n",
    "    ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "    epoch = 0\n",
    "    store_epoch_loss = []\n",
    "    store_qwk_epoch_loss = []\n",
    "    store_epoch_loss_val = []\n",
    "    store_qwk_epoch_loss_val = []\n",
    "    store_epoch_acc_val = []\n",
    "    try:\n",
    "        for e in tqdm(range(EPOCHS)):\n",
    "            #scheduler.step()\n",
    "            epoch = e + 1\n",
    "            epoch_loss = 0\n",
    "            qwk_epoch_loss = 0\n",
    "            store_batch_loss = []\n",
    "            store_qwk_batch_loss = []\n",
    "            \n",
    "            for batch_num, (X, y) in enumerate(data_train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model.forward(X.cuda())\n",
    "                batch_loss = ce_loss(prediction, y)\n",
    "                batch_loss.backward()\n",
    "                qwk_batch_loss = qwk_loss(y.clone().detach().cpu().numpy(), \n",
    "                                          np.argmax(prediction.clone().detach().cpu().numpy(), axis=1), \n",
    "                                          weights=\"quadratic\")\n",
    "                optimizer.step()\n",
    "                store_batch_loss.append(batch_loss.clone().cpu())\n",
    "                store_qwk_batch_loss.append(qwk_batch_loss)\n",
    "                epoch_loss = torch.FloatTensor(store_batch_loss).mean()\n",
    "                qwk_epoch_loss = torch.FloatTensor(store_qwk_batch_loss).mean()\n",
    "                \n",
    "            store_epoch_loss.append(epoch_loss)\n",
    "            store_qwk_epoch_loss.append(qwk_epoch_loss)\n",
    "            torch.save(model.state_dict(), \"{}/checkpoint_{}.pth\".format(training_dir, epoch))\n",
    "#             plt.plot(store_epoch_loss[1:], label=\"Training Loss\")\n",
    "#             plt.plot(store_qwk_epoch_loss[1:], label=\"Training Metric(QWK)\")\n",
    "\n",
    "            model.eval()\n",
    "            epoch_loss_val = 0\n",
    "            qwk_epoch_loss_val = 0\n",
    "            epoch_acc_val = 0\n",
    "            store_batch_loss_val = []\n",
    "            store_qwk_batch_loss_val = []\n",
    "            store_batch_acc_val = []\n",
    "            misclassified_images = []\n",
    "            for batch_num, (X, y) in enumerate(data_val_loader):\n",
    "                with torch.no_grad():\n",
    "                    prediction = model.forward(X.cuda())\n",
    "                batch_loss = ce_loss(prediction, y)\n",
    "                qwk_batch_loss = qwk_loss(y.clone().detach().cpu().numpy(), \n",
    "                                          np.argmax(prediction.clone().detach().cpu().numpy(), axis=1), \n",
    "                                          weights=\"quadratic\")\n",
    "                misclassified = prediction.max(-1)[-1].squeeze().cpu() != y.cpu()\n",
    "                misclassified_images.append(X[misclassified==1].cpu())\n",
    "                batch_acc = misclassified.float().mean()\n",
    "                store_batch_loss_val.append(batch_loss)\n",
    "                store_qwk_batch_loss_val.append(qwk_batch_loss)\n",
    "                store_batch_acc_val.append(batch_acc)\n",
    "                epoch_loss_val = torch.FloatTensor(store_batch_loss_val).mean()\n",
    "                qwk_epoch_loss_val = torch.FloatTensor(store_qwk_batch_loss_val).mean()\n",
    "                epoch_acc_val = torch.FloatTensor(store_batch_acc_val).mean()\n",
    "            store_epoch_loss_val.append(epoch_loss_val)\n",
    "            store_qwk_epoch_loss_val.append(qwk_epoch_loss_val)\n",
    "            store_epoch_acc_val.append(1-epoch_acc_val)\n",
    "            plt.plot(store_epoch_loss_val[1:], label=\"Validation Loss\")\n",
    "            plt.plot(store_qwk_epoch_loss_val[1:], label=\"Validation Metric(QWK)\")\n",
    "            plt.plot(store_epoch_acc_val[1:], label=\"Validation Accuracy\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.savefig(\"{}/Loss.png\".format(training_dir))\n",
    "            plt.close()\n",
    "            if len(misclassified_images) > 0:\n",
    "                misclassified_images = np.concatenate(misclassified_images,axis=0)\n",
    "                validation_dir = training_dir+'/misclassified/checkpoint_{}'.format(epoch)\n",
    "                os.mkdir(validation_dir)\n",
    "            model.train()\n",
    "        most_acc = max(store_epoch_acc_val)\n",
    "        min_loss = min(store_epoch_loss_val)\n",
    "        qwk_max_loss = max(store_qwk_epoch_loss_val)\n",
    "        print(\"\\nHighest accuracy of {} occured at {}...\\nMinimum loss occured at {}... \\nMaximum QWK metric of {} occured at {}\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1, \n",
    "            store_epoch_loss_val.index(min_loss)+1, \n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "        with open(training_dir+\"/HYP.txt\",\"w+\") as f:\n",
    "            f.write(\"EPOCH = {} \\n\".format(EPOCHS))\n",
    "            f.write(\"LR = {} \\n\".format(LR))\n",
    "            f.write(\"HIDDEN_LAYERS = {} \\n\".format(HIDDEN))\n",
    "            f.write(\"ONE_HOT = {} \\n\".format(ONE_HOT))\n",
    "            f.write(\"DATA_AUG = {} \\n\".format(DATA_AUG))\n",
    "            f.write(\"Highest accuracy of {} occured at {}...\\nMinimum loss of {} occured at {}... \\nMaximum QWK metric of {} occured at {}\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1, \n",
    "            min_loss, store_epoch_loss_val.index(min_loss)+1, \n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "        checkpoints = os.listdir(training_dir)\n",
    "        for checkpoint in checkpoints:\n",
    "            if \"checkpoint\" in checkpoint:\n",
    "                checkpoint_num = int(checkpoint[checkpoint.index(\"_\")+1:checkpoint.index(\".\")])\n",
    "                if checkpoint_num not in [store_qwk_epoch_loss_val.index(qwk_max_loss)+1,\n",
    "                                          store_epoch_loss_val.index(min_loss)+1,\n",
    "                                          store_epoch_acc_val.index(most_acc)+1]:\n",
    "                    os.remove(training_dir+\"/\"+checkpoint)        \n",
    "    except KeyboardInterrupt:\n",
    "        most_acc = max(store_epoch_acc_val)\n",
    "        min_loss = min(store_epoch_loss_val)\n",
    "        qwk_max_loss = max(store_qwk_epoch_loss_val)\n",
    "        print(\"\\nHighest accuracy of {} occured at {}...\\nMinimum loss of {} occured at {}... \\nMaximum QWK metric of {} occured at {}\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1, \n",
    "            min_loss, store_epoch_loss_val.index(min_loss)+1, \n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "        with open(training_dir+\"/HYP.txt\",\"w+\") as f:\n",
    "            f.write(\"EPOCH = {} \\n\".format(EPOCHS))\n",
    "            f.write(\"LR = {} \\n\".format(LR))\n",
    "            f.write(\"HIDDEN_LAYERS = {} \\n\".format(HIDDEN))\n",
    "            f.write(\"ONE_HOT = {} \\n\".format(ONE_HOT))\n",
    "            f.write(\"DATA_AUG = {} \\n\".format(DATA_AUG))\n",
    "            f.write(\"Highest accuracy of {} occured at {}...\\nMinimum loss of {} occured at {}... \\nMaximum QWK metric of {} occured at {}\\n\".format(\n",
    "            most_acc, store_epoch_acc_val.index(most_acc)+1,\n",
    "            min_loss, store_epoch_loss_val.index(min_loss)+1,\n",
    "            qwk_max_loss, store_qwk_epoch_loss_val.index(qwk_max_loss)+1))\n",
    "            f.write(\"TRAINING INCOMPLETE\")\n",
    "        checkpoints = os.listdir(training_dir)\n",
    "        for checkpoint in checkpoints:\n",
    "            if \"checkpoint\" in checkpoint:\n",
    "                checkpoint_num = int(checkpoint[checkpoint.index(\"_\")+1:checkpoint.index(\".\")])\n",
    "                if checkpoint_num not in [store_qwk_epoch_loss_val.index(qwk_max_loss)+1,\n",
    "                                          store_epoch_loss_val.index(min_loss)+1,\n",
    "                                          store_epoch_acc_val.index(most_acc)+1]:\n",
    "                    os.remove(training_dir+\"/\"+checkpoint)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     for ONE_HOT in [0,1]: # for MaturitySize, FurLength, Health\n",
    "#         for DATA_AUG in [0,1]: # for state data\n",
    "    data_train_loader, data_val_loader = prep_dataset(0, 0)\n",
    "    train(Model(HIDDEN_LIST[2], 0, 0).cuda(), HIDDEN_LIST[2], 0, 0, data_train_loader, data_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_v1]",
   "language": "python",
   "name": "conda-env-pytorch_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
